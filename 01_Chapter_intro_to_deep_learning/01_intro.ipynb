{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About the Book\n",
    "\n",
    "Deep Learning with Python\" is a book written by François Chollet, who is the creator of the Keras deep learning library. The book serves as a comprehensive introduction to deep learning and its practical applications, with a focus on using Python and the Keras library.\n",
    "\n",
    "The book starts with an introduction to the fundamental concepts of deep learning, including neural networks, convolutional neural networks (CNNs), recurrent neural networks (RNNs), and generative models. The author then goes on to explain the basics of the Keras library, including how to build, train, and evaluate deep learning models.\n",
    "\n",
    "The book covers a wide range of topics in deep learning, including image classification, object detection, natural language processing, and reinforcement learning. It also includes case studies and examples of real-world applications of deep learning.\n",
    "\n",
    "One of the best reason for reading this book that it is written in a clear and concise style that makes it easy for beginners to understand. It also includes numerous code examples and practical exercises, which help readers to apply the concepts they have learned.\n",
    "\n",
    "Overall, \"Deep Learning with Python\" is a valuable resource for anyone interested in learning about deep learning and its applications. It is particularly useful for those who want to use Python and the Keras library to build their own deep learning models. If you want to walk with me with this journey, let walk together."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial intelligence, Machine Learning and Deep learning.\n",
    "\n",
    "![](https://master-iesc-angers.com/wp-content/uploads/2018/04/2018-04-13_1322.png)\n",
    "\n",
    "\n",
    "#### Artificial Intelligence\n",
    "The goal of Artificial Intelligence (AI) is to create systems that can perform tasks that require human intelligence, such as understanding natural language, recognizing objects in images or videos, playing games, making decisions, and solving problems. The field of AI is very broad and includes many different techniques and approaches, but machine learning and deep learning are two of the most important and widely used subfields. Short history of AI is given below:\n",
    "\n",
    "- Artificial intelligence (AI) emerged in the 1950s as a field of research, with pioneers in computer science exploring whether computers could be made to \"think.\"\n",
    "- John McCarthy organized a summer workshop in 1956 that set the groundwork for AI research by proposing that every aspect of learning or intelligence could be precisely described so that machines could simulate it.\n",
    "- The workshop did not fully solve the problems it set out to investigate, but it brought together many people who became pioneers in the field and sparked an ongoing intellectual revolution.\n",
    "- AI is the effort to automate intellectual tasks that humans typically perform and encompasses approaches beyond machine learning and deep learning.\n",
    "- Until the 1980s, most AI textbooks did not mention \"learning,\" and early chess programs were based on hardcoded rules instead of machine learning.\n",
    "- The dominant paradigm in AI from the 1950s to the late 1980s was symbolic AI, which involved handcrafting a large set of explicit rules for manipulating knowledge in databases.\n",
    "- Symbolic AI was suitable for solving well-defined, logical problems like chess, but it was intractable for more complex problems like image classification, speech recognition, and natural language translation.\n",
    "- Machine learning emerged as a new approach to replace symbolic AI because it allows computers to learn from data rather than relying on explicit rules.\n",
    "\n",
    "#### Machine Learning\n",
    "Machine learning is a subfield of artificial intelligence that involves building algorithms and models that can learn patterns and insights from data, without being explicitly programmed to do so. The book defines machine learning as \"the ability of a computer to learn from data, without being explicitly programmed\".\n",
    "\n",
    "In machine learning, a model is trained on a large set of data, and it learns to make predictions or take actions based on that data. The training process involves adjusting the parameters of the model to minimize the difference between the model's predictions and the true values in the training data. Once the model is trained, it can be used to make predictions on new, unseen data.\n",
    "\n",
    "Short history of Machine learning:\n",
    "\n",
    "- Lady Ada Lovelace was a friend and collaborator of Charles Babbage, the inventor of the Analytical Engine, which was the first-known general-purpose mechanical computer.\n",
    "- The Analytical Engine was not meant as a general-purpose computer when it was designed in the 1830s and 1840s because the concept of general-purpose computation was yet to be invented.\n",
    "- The Pascaline, designed by Blaise Pascal in 1642, was the world’s first mechanical calculator.\n",
    "- In 1843, Ada Lovelace remarked on the invention of the Analytical Engine, saying that it had no pretensions whatever to originate anything and its province is to assist us in making available what we already know.\n",
    "- AI pioneer Alan Turing quoted Lady Lovelace’s remark as “Lady Lovelace’s objection” in his landmark 1950 paper “Computing Machinery and Intelligence,” in which he introduced the Turing test as well as key concepts that would come to shape AI.\n",
    "- Turing was of the opinion that computers could in principle be made to emulate all aspects of human intelligence.\n",
    "- The usual way to make a computer do useful work is to have a human programmer write down rules; machine learning turns this around by having the machine learn from input data and corresponding answers to figure out what the rules should be.\n",
    "- Machine learning is trained rather than explicitly programmed; it is presented with many examples relevant to a task, and it finds statistical structure in these examples that eventually allows the system to come up with rules for automating the task.\n",
    "- Machine learning only started to flourish in the 1990s, driven by the availability of faster hardware and larger datasets. It has quickly become the most popular and most successful subfield of AI.\n",
    "- Machine learning deals with large, complex datasets for which classical statistical analysis would be impractical.\n",
    "- Machine learning is fundamentally an engineering discipline and is a very hands-on field driven by empirical findings and deeply reliant on advances in software and hardware.\n",
    "- To do machine learning, input data points, examples of the expected output, and a way to measure whether the algorithm is doing a good job are required. The adjustment step based on the feedback signal is what we call learning.\n",
    "\n",
    "\n",
    "#### Deep Learning \n",
    "Deep learning is a type of machine learning that emphasizes learning successive layers of increasingly meaningful representations. These layered representations are learned through neural networks, which are structured in literal layers stacked on top of each other. Deep learning models typically involve tens or even hundreds of successive layers of representations learned automatically from exposure to training data. The depth of a model refers to how many layers contribute to a model of the data. Deep learning is not a model of the brain, despite some inspiration from neurobiology. Instead, it is a mathematical framework for learning representations from data.\n",
    "\n",
    "![deep learning](./assets/1.PNG)\n",
    "\n",
    "Deep learning is a way to learn data representations in a multistage process. The network uses successive filters to transform the original image into increasingly informative representations that are useful for some task. This process can be thought of as an information distillation process, where information is purified through each stage. Despite its simplicity, when scaled appropriately, deep learning can appear like magic.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding how deep learning works, in three figures\n",
    "\n",
    "![](./assets/3.PNG)\n",
    "\n",
    "1. The layers of a neural network transform the input data through simple data transformations that are parameterized by the layer's weights. Learning means finding the right values for all of the weights in the network, which can be a daunting task because there can be tens of millions of parameters that all affect each other.\n",
    "\n",
    "![](./assets/4.PNG)\n",
    "\n",
    "2. In order to control the output of a neural network, it is important to have a way to measure the distance between the predicted output and the expected output. This is done using a loss function, which takes in the network's predictions and the true target values and computes a score that represents the network's performance on the given example. The loss function is sometimes also referred to as the objective function or cost function\n",
    "\n",
    "![](./assets/5.PNG)\n",
    "\n",
    "\n",
    "3. The key to deep learning is to use the loss function, which measures the distance between the predicted output and the true target, as a feedback signal to adjust the weights of the network. This adjustment is done by the optimizer using the Backpropagation algorithm. Initially, the weights are assigned random values and the network output is far from ideal, resulting in a high loss score. However, with each example, the weights are adjusted in the correct direction, and the loss score decreases. This is the training loop, which eventually yields weight values that minimize the loss function and produce a trained network. It's a simple mechanism that looks like magic once scaled."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A brif history of machine learning \n",
    "\n",
    "Probabilistic Methods: \n",
    "- Probabilistic modeling is a form of machine learning that applies statistical principles to data analysis, and it has been around for a long time. Naive Bayes is a well-known algorithm in this category, which is a type of machine learning classifier based on applying Bayes' theorem while assuming that the features in the input data are all independent. Logistic regression is another closely related model that is often considered the \"Hello World\" of modern machine learning and is a useful classification algorithm. These methods are still widely used today due to their simple and versatile nature.\n",
    "\n",
    "Early Neural Nets:\n",
    "- The core ideas of neural networks were investigated in the 1950s, but an efficient way to train large neural networks was missing. This changed in the mid-1980s when multiple people independently rediscovered the Backpropagation algorithm, which is a way to train chains of parametric operations using gradient-descent optimization. The first successful practical application of neural nets came in 1989 when Yann LeCun combined the earlier ideas of convolutional neural networks and backpropagation and applied them to the problem of classifying handwritten digits. The resulting network, dubbed LeNet, was used by the United States Postal Service in the 1990s to automate the reading of ZIP codes on mail envelopes.\n",
    "\n",
    "Kernel Methods:\n",
    "- Kernel methods, including the Support Vector Machine (SVM), rose to fame in the 1990s and quickly sent neural nets back to oblivion.\n",
    "SVM is a classification algorithm that works by finding decision boundaries separating two classes.\n",
    "- SVMs find decision boundaries by mapping the data to a new high-dimensional representation where the decision boundary can be expressed as a hyperplane, then computing a good decision boundary by maximizing the margin.\n",
    "- To find good decision hyperplanes in the new representation space, the kernel trick is used where a kernel function is used to compute the distance between pairs of points in that space.\n",
    "- SVMs exhibited state-of-the-art performance on simple classification problems and were easily interpretable but were hard to scale to large datasets and didn't provide good results for perceptual problems like image classification.\n",
    "- Applying an SVM to perceptual problems requires first extracting useful representations manually (a step called feature engineering), which is difficult and brittle.\n",
    "\n",
    "Decision trees, random forests, and gradient boosting machines:\n",
    "- Decision trees are flowchart-like structures that can classify input data points or predict output values given inputs. They became popular in the 2000s and by 2010 were often preferred over kernel methods. Random forests, which involve building a large number of specialized decision trees and then ensembling their outputs, are applicable to a wide range of problems and are almost always the second-best algorithm for any shallow machine learning task. Gradient boosting machines, which use gradient boosting to improve any machine learning model by iteratively training new models that specialize in addressing the weak points of the previous models, are now considered one of the best algorithms for dealing with nonperceptual data\n",
    "\n",
    "\n",
    "Rise of neural nets:\n",
    "- In 2010, neural networks were shunned by the scientific community, but breakthroughs were being made by groups such as Hinton, Bengio, LeCun, and IDSIA.\n",
    "- In 2011, Dan Ciresan from IDSIA won academic image-classification competitions with GPU-trained deep neural networks.\n",
    "- The watershed moment came in 2012 with Hinton's group entry in the ImageNet challenge, achieving a top-five accuracy of 83.6% with deep convolutional neural networks (convnets).\n",
    "- By 2015, the winner of the ImageNet challenge reached an accuracy of 96.4%, and convnets became the go-to algorithm for all computer vision tasks.\n",
    "- Deep learning has also found applications in many other types of problems, such as natural language processing, and has replaced SVMs and decision trees in a wide range of applications.\n",
    "\n",
    "Deep learning has quickly become a popular method for solving complex problems in machine learning because it offers better performance and completely automates feature engineering, which is the most crucial step in a machine learning workflow. Shallow learning techniques require humans to manually engineer good layers of representations for their data, while deep learning learns all features in one pass, greatly simplifying the machine learning process. Successive applications of shallow-learning methods produce fast-diminishing returns, making deep learning's joint feature learning method much more powerful. The incremental, layer-by-layer way in which increasingly complex representations are developed, and the fact that these intermediate incremental representations are learned jointly, have made deep learning much more successful than previous approaches to machine learning. In machine learning competitions on Kaggle, deep learning methods and gradient boosted trees are the primary software tools used by top teams.\n",
    "\n",
    "![](./assets/6.PNG)\n",
    "\n",
    "\n",
    "Why Deep learning now:\n",
    "- Powerful Hardware\n",
    "- Companies generating more data than ever\n",
    "- State of the art new algorithms\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still feeling curiosity? Explore these questions\n",
    "1. Can we develop deep learning models that can truly understand and reason about the world, beyond just pattern recognition?\n",
    "2. How can we make deep learning models more explainable and interpretable?\n",
    "3. Can we create deep learning models that are able to learn from fewer examples, similar to how humans can learn new concepts quickly?\n",
    "4. How can we apply deep learning to solve more complex and abstract problems, such as natural language understanding, emotion recognition, and creativity?\n",
    "5. Can we develop deep learning models that are more robust and resilient to adversarial attacks?\n",
    "6. How can we make deep learning models more adaptive to changing environments and data distributions?\n",
    "7. How can we ensure that deep learning models are ethically and socially responsible, and do not reinforce or amplify existing biases and inequalities?\n",
    "8. Can we create deep learning models that can work with less data and less computing power, to make deep learning more accessible and sustainable?\n",
    "9. How can we combine deep learning with other AI techniques, such as symbolic reasoning, reinforcement learning, and evolutionary computation, to create more powerful and flexible AI systems?\n",
    "10. Can we develop deep learning models that can work together and learn from each other, to create more collaborative and cooperative AI systems?\n",
    "\n",
    "\n",
    "___\n",
    "References: \n",
    "*- Deep Learning with Python\" by François Chollet (Manning Publications)*\n",
    "\n",
    "*- Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press)*\n",
    "\n",
    "*- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\" by Aurélien Géron (O'Reilly Media)*\n",
    "\n",
    "*- Neural Networks and Deep Learning: A Textbook\" by Charu Aggarwal (Springer)*\n",
    "\n",
    "*- Deep Learning for Computer Vision: Expert techniques to train advanced neural networks using TensorFlow and Keras\" by Rajalingappaa Shanmugamani (Packt Publishing)*\n",
    "\n",
    "*- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition. by Aurélien Géron. Released September 2019. Publisher(s): O'Reilly Media*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
