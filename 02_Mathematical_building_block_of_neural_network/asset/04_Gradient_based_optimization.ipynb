{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Based Optimization: \n",
    "In machine learning, the goal is to train a model that can accurately predict outcomes based on input data. This training process involves repeatedly showing the model examples of input-output pairs (also called samples or examples), adjusting the model's weights based on how well it performs on those examples, and iterating until the model's performance is satisfactory.\n",
    "\n",
    "This happens within what’s called a training loop, which works as follows. Repeat these steps in a loop, until the loss seems sufficiently low:\n",
    "1. Draw a batch of training samples, x, and corresponding targets, y_true.\n",
    "2.  Run the model on x (a step called the forward pass) to obtain predictions, y_pred.\n",
    "3.  Compute the loss of the model on the batch, a measure of the mismatch between y_pred and y_true.\n",
    "4.  Update all weights of the model in a way that slightly reduces the loss on this batch\n",
    "\n",
    "The difficult part is step 4: updating the model’s weights.\n",
    "\n",
    "The process of adjusting the weights involves computing the gradient of the model's loss function with respect to the weights, and then using this gradient to update the weights in a way that reduces the loss. This is typically done using an optimization algorithm such as Stochastic Gradient Descent (SGD)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "Stochastic gradient descent (SGD) is a widely used optimization algorithm for training machine learning models, including neural networks. It is an iterative method that updates the model's parameters, such as the weights, based on the gradients of the loss function with respect to those parameters. In essence, SGD finds the optimal values of the model parameters that minimize the loss function.\n",
    "\n",
    "Instead of computing the gradients over the entire training set, as in batch gradient descent, SGD computes the gradients over a small random subset of the training set, or mini-batch, at each iteration. This allows for faster training on large datasets, as well as the ability to escape local minima and saddle points that can occur in the loss surface. The learning rate, which controls the step size of the parameter updates, can be adjusted dynamically during training to improve convergence speed and stability.\n",
    "\n",
    "SGD has several variants, including momentum, Adagrad, RMSprop, and others, which adaptively adjust the learning rate or incorporate momentum terms to improve convergence and reduce oscillations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Backpropagation algorithm\n",
    "\n",
    "Backpropagation is an algorithm that computes the gradient of the loss function with respect to the weights and biases in a neural network. It works by using the chain rule of calculus to compute the derivative of a function that is composed of other functions. Specifically, it computes the derivative of a composite function by propagating the error backwards through the network and computing the partial derivative of the loss function with respect to each weight and bias.\n",
    "\n",
    "Backpropagation involves two passes through the network: a forward pass and a backward pass. In the forward pass, the input is fed through the network, and the output is computed. In the backward pass, the error is propagated backwards through the network, and the partial derivatives of the loss function with respect to each weight and bias are computed.\n",
    "\n",
    "Backpropagation is based on the idea of computation graphs, which are directed acyclic graphs of operations that represent the computations performed by the network. The graph is traversed in reverse order during the backward pass, and the partial derivatives are computed using the chain rule. The derivatives are then used to update the weights and biases using an optimization algorithm, such as stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
